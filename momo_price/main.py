import os
import sys
import re
import time
import json
import base64
import requests
from urllib.parse import urlparse, parse_qs, unquote
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# --- 1. åˆå§‹åŒ– ---
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")

try:
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("âš ï¸ è­¦å‘Š: æœªåµæ¸¬åˆ° Supabase ç’°å¢ƒè®Šæ•¸")
        supabase = None
    else:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
except Exception as e:
    print(f"âŒ Supabase åˆå§‹åŒ–å¤±æ•—: {e}")
    sys.exit(1)

# --- 2. å·¥å…·å‡½å¼ ---

def decode_base64_safe(data):
    if not data: return ""
    try:
        return base64.b64decode(data).decode('utf-8')
    except:
        return data

def extract_inner_url(url):
    """å¾ä¸­è½‰é€£çµ (reurl.jsp) æå–çœŸæ­£çš„å•†å“é€£çµ"""
    if not url: return None
    if "goodsUrl=" in url:
        try:
            parsed = urlparse(url)
            params = parse_qs(parsed.query)
            if 'goodsUrl' in params:
                return unquote(params['goodsUrl'][0])
        except Exception as e:
            print(f"âš ï¸ è§£æå…§éƒ¨é€£çµå¤±æ•—: {e}")
    return url

def normalize_momo_url(url):
    """
    ã€V10.12 ä¿®æ­£ã€‘æ™ºæ…§ç¶²å€æ¨™æº–åŒ–
    1. ç´”æ•¸å­— ID -> è½‰æ¨™æº–é é¢ (JSON-LD æœ€å®Œæ•´)
    2. TP é–‹é ­ ID -> ä¿æŒåŸæ¨£ (é¿å…æ­»æª”)
    """
    if not url: return None
    
    # å˜—è©¦æŠ“å– ID
    match = re.search(r'goodsDetail/([A-Za-z0-9]+)', url)
    if match:
        product_id = match.group(1)
        
        # â˜… é—œéµä¿®æ­£: å¦‚æœæ˜¯ TP é–‹é ­ï¼Œçµ•å°ä¸è¦å‹•å®ƒï¼
        if product_id.startswith("TP"):
            print(f"â„¹ï¸ åµæ¸¬åˆ°æ´»å‹•å•†å“ (TP)ï¼Œä¿æŒç‰¹æ®Šç¶²å€çµæ§‹")
            return url
            
        # åªæœ‰ç´”æ•¸å­— ID æ‰è½‰æ¨™æº–ç¶²å€
        if product_id.isdigit():
            standard_url = f"https://www.momoshop.com.tw/goods/GoodsDetail.jsp?i_code={product_id}"
            print(f"ğŸ”§ æ¨™æº–åŒ–å•†å“ç¶²å€: {standard_url}")
            return standard_url
        
    return url

def resolve_short_url(url):
    if not url: return None
    if "momoshop.com.tw/goods/GoodsDetail" in url and "reurl.jsp" not in url:
        return url
        
    print(f"ğŸ”„ æ­£åœ¨é‚„åŸçŸ­ç¶²å€: {url} ...")
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10, allow_redirects=True)
        final_url = response.url
        inner_url = extract_inner_url(final_url)
        normalized_url = normalize_momo_url(inner_url) # é€™è£¡æœƒåŸ·è¡Œæ™ºæ…§åˆ¤æ–·
        return normalized_url
        
    except Exception as e:
        print(f"âš ï¸ é‚„åŸç¶²å€å¤±æ•—ï¼Œå°‡ä½¿ç”¨åŸç¶²å€: {e}")
        return url

def extract_url_from_text(text):
    if not text: return None
    decoded_text = decode_base64_safe(text)
    print(f"ğŸ“¦ è§£ç¢¼å¾Œå…§å®¹: {decoded_text}") 
    match = re.search(r'(https?://[^\s]+)', decoded_text)
    if match: return match.group(1)
    return decoded_text

def clean_price_text(text):
    if not text: return None
    clean = re.sub(r'[^\d]', '', str(text))
    return int(clean) if clean else None

def extract_json_ld(soup, platform):
    scripts = soup.find_all('script', type='application/ld+json')
    for script in scripts:
        try:
            data = json.loads(script.string)
            if isinstance(data, list):
                for item in data:
                    if item.get('@type') == 'Product': return item
            elif isinstance(data, dict):
                if data.get('@type') == 'Product': return data
        except: continue
    return None

# --- 3. è§£æé‚è¼¯ ---

def parse_momo(soup):
    price, title = None, "Momoå•†å“"
    
    # 1. JSON-LD (å„ªå…ˆ)
    json_data = extract_json_ld(soup, "momo")
    if json_data:
        if 'offers' in json_data and 'price' in json_data['offers']:
            price = clean_price_text(json_data['offers']['price'])
        if 'name' in json_data: title = json_data['name']

    # 2. è¦–è¦ºæ¨™ç±¤ (å…¨é¢è¦†è“‹ä¸€èˆ¬é èˆ‡æ´»å‹•é )
    if not price:
        selectors = [
            # æ¨™æº–é é¢
            "span.price", "span.seoPrice", "ul.price li.special span.price b",
            ".priceArea .price", ".special .price", ".product_price b",
            
            # â˜… V10.12 æ–°å¢: é‡å° TP æ´»å‹•é çš„å¸¸è¦‹çµæ§‹
            ".goodsPrice .price", 
            ".d-price .price", 
            "dd.price b", 
            ".amount",
            ".checkoutPrice"
        ]
        for sel in selectors:
            tag = soup.select_one(sel)
            if tag:
                price = clean_price_text(tag.text)
                if price: break

    # 3. æ¨™é¡Œ
    if title == "Momoå•†å“":
        og_title = soup.find("meta", property="og:title")
        title = og_title["content"] if og_title else (soup.title.text.split("- momo")[0].strip() if soup.title else title)

    return price, title

def parse_pchome(soup):
    price, title = None, "PChomeå•†å“"
    json_data = extract_json_ld(soup, "pchome")
    if json_data:
        if 'offers' in json_data:
            offers = json_data['offers']
            if isinstance(offers, dict) and 'price' in offers: price = clean_price_text(offers['price'])
            elif isinstance(offers, list) and offers and 'price' in offers[0]: price = clean_price_text(offers[0]['price'])
        if 'name' in json_data: title = json_data['name']
        if price: return price, title

    if not price:
        meta = soup.find("meta", property="product:price:amount") or soup.find("meta", property="og:price:amount")
        if meta: price = clean_price_text(meta["content"])
    
    if not price:
        for sel in ["#PriceTotal", ".o-prodPrice__price", ".price-info__price", "span[id^='PriceTotal']"]:
            tag = soup.select_one(sel)
            if tag: 
                price = clean_price_text(tag.text)
                if price: break

    if title == "PChomeå•†å“":
        name_tag = soup.find(id="NickName")
        title = name_tag.text.strip() if name_tag else (soup.title.text.split("- PChome")[0].strip() if soup.title else title)

    return price, title

def get_product_info(base64_str):
    raw_url = extract_url_from_text(base64_str)
    real_url = resolve_short_url(raw_url)
    
    print(f"ğŸ” æº–å‚™é€£ç·š: {real_url}")
    
    platform = "unknown"
    if "momoshop.com.tw" in real_url: platform = "momo"; print("ğŸ’¡ è­˜åˆ¥ç‚º: Momo")
    elif "pchome.com.tw" in real_url: platform = "pchome"; print("ğŸ’¡ è­˜åˆ¥ç‚º: PChome")

    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled") 
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")

    driver = webdriver.Chrome(options=chrome_options)
    try:
        driver.get(real_url)
        time.sleep(5)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        if platform == "momo": return parse_momo(soup)
        elif platform == "pchome": return parse_pchome(soup)
        else: return parse_momo(soup)
    except Exception as e:
        print(f"âŒ çˆ¬èŸ²éŒ¯èª¤: {e}")
        return None, None
    finally:
        driver.quit()

# --- 4. å„²å­˜ ---

def save_price_record(user_id, raw_url_or_text, price, title):
    if not supabase: return
    print(f"ğŸ’¾ å„²å­˜ä¸­: {title} | ${price}")
    try:
        clean_url = extract_url_from_text(raw_url_or_text)
        real_url = resolve_short_url(clean_url)

        product_data = {
            "user_id": user_id,
            "original_url": real_url,
            "current_price": price,
            "product_name": title,
            "is_active": True,
            "updated_at": "now()"
        }
        existing = supabase.table("products").select("id").eq("original_url", real_url).eq("user_id", user_id).execute()
        
        if existing.data:
            pid = existing.data[0]['id']
            supabase.table("products").update(product_data).eq("id", pid).execute()
        else:
            res = supabase.table("products").insert(product_data).execute()
            pid = res.data[0]['id'] if res.data else None

        if pid:
            supabase.table("price_history").insert({"product_id": pid, "price": price, "recorded_at": "now()"}).execute()
            print("âœ… æˆåŠŸ")
    except Exception as e:
        print(f"âŒ å¯«å…¥å¤±æ•—: {e}")

if __name__ == "__main__":
    if len(sys.argv) > 2:
        raw_msg = sys.argv[1]
        uid = sys.argv[2]
        
        print("ğŸš€ V10.12 æ™ºæ…§é›™è»Œåˆ¶å•Ÿå‹•...")
        
        price, title = get_product_info(raw_msg)
        if price:
            save_price_record(uid, raw_msg, price, title)
        else:
            print("âŒ å¤±æ•—: ç„¡æ³•æŠ“å–")
    else:
        print("âŒ åƒæ•¸ä¸è¶³")
