import os
import sys
import re
import time
import json
import base64
import requests
from urllib.parse import urlparse, parse_qs, unquote
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# --- 1. åˆå§‹åŒ– ---
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")

try:
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("âš ï¸ è­¦å‘Š: æœªåµæ¸¬åˆ° Supabase ç’°å¢ƒè®Šæ•¸")
        supabase = None
    else:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
except Exception as e:
    print(f"âŒ Supabase åˆå§‹åŒ–å¤±æ•—: {e}")
    sys.exit(1)

# --- 2. å·¥å…·å‡½å¼ ---

def decode_base64_safe(data):
    if not data: return ""
    try:
        return base64.b64decode(data).decode('utf-8')
    except:
        return data

def extract_inner_url(url):
    if not url: return None
    if "goodsUrl=" in url:
        try:
            parsed = urlparse(url)
            params = parse_qs(parsed.query)
            if 'goodsUrl' in params:
                return unquote(params['goodsUrl'][0])
        except: pass
    return url

def normalize_momo_url(url):
    if not url: return None
    match = re.search(r'goodsDetail/([A-Za-z0-9]+)', url)
    if match:
        product_id = match.group(1)
        if product_id.startswith("TP"): return url
        if product_id.isdigit():
            return f"https://www.momoshop.com.tw/goods/GoodsDetail.jsp?i_code={product_id}"
    return url

def resolve_short_url(url):
    if not url: return None
    if "momoshop.com.tw/goods/GoodsDetail" in url and "reurl.jsp" not in url:
        return url
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10, allow_redirects=True)
        final_url = response.url
        inner_url = extract_inner_url(final_url)
        return normalize_momo_url(inner_url)
    except Exception as e:
        return url

def extract_url_from_text(decoded_text):
    if not decoded_text: return None
    match = re.search(r'(https?://[^\s]+)', decoded_text)
    if match: return match.group(1)
    return decoded_text

def clean_price_text(text):
    if not text: return None
    clean = re.sub(r'[^\d]', '', str(text))
    if not clean: return None
    return int(clean)

def extract_price_from_user_text(text):
    if not text: return None
    # å¾æ–‡å­—æå–æ™‚ï¼Œå˜—è©¦æ‰¾æœ€ä½çš„åˆç†åƒ¹æ ¼ (é€šå¸¸æ–‡å­—è£¡ä¹Ÿæœƒæœ‰å®šåƒ¹å’Œç‰¹åƒ¹)
    candidates = []
    patterns = [r'ã€(\d+(?:,\d+)*)å…ƒ', r'\$(\d+(?:,\d+)*)', r'(\d+(?:,\d+)*)å…ƒ']
    for p in patterns:
        matches = re.finditer(p, text)
        for m in matches:
            val = clean_price_text(m.group(1))
            if val and val > 100: 
                candidates.append(val)
    
    if candidates:
        return min(candidates) # å‡è¨­æ–‡å­—è£¡æœ‰ "åŸåƒ¹3980 ç‰¹åƒ¹3680"ï¼Œæˆ‘å€‘å– 3680
    return None

def extract_json_ld(soup, platform):
    scripts = soup.find_all('script', type='application/ld+json')
    for script in scripts:
        try:
            data = json.loads(script.string)
            if isinstance(data, list):
                for item in data:
                    if item.get('@type') == 'Product': return item
            elif isinstance(data, dict):
                if data.get('@type') == 'Product': return data
        except: continue
    return None

# --- 3. è§£æé‚è¼¯ (V10.18 æ ¸å¿ƒå‡ç´š: ä¿ƒéŠ·åƒ¹ç‹™æ“Šæ‰‹) ---

def parse_momo(soup):
    title = "Momoå•†å“"
    
    # æ¨™é¡Œ
    og_title = soup.find("meta", property="og:title")
    title = og_title["content"] if og_title else (soup.title.text.split("- momo")[0].strip() if soup.title else title)

    # === ç­–ç•¥ A: è¦–è¦º CSS ä¿ƒéŠ·å€å¡Š (Level 1 - æœ€å„ªå…ˆ) ===
    # æˆ‘å€‘ä¸å†åªçœ‹ span.priceï¼Œæˆ‘å€‘è¦çœ‹å®ƒæ˜¯ä¸æ˜¯åœ¨ "special" (ä¿ƒéŠ·) å€å¡Šè£¡
    # é€™æ˜¯ Momo æœ€å…¸å‹çš„ç‰¹åƒ¹çµæ§‹: <li class="special"> <span>ä¿ƒéŠ·åƒ¹</span> <span class="price">3,680</span> </li>
    
    promo_selectors = [
        "ul.price li.special span.price",  # æ¨™æº–ç‰¹åƒ¹å€
        ".priceArea .price",               # æ–°ç‰ˆç‰¹åƒ¹å€
        ".product_price .price",           # å¦ä¸€ç¨®çµæ§‹
        "b.price"                          # å¼·èª¿çš„åƒ¹æ ¼
    ]

    for sel in promo_selectors:
        tag = soup.select_one(sel)
        if tag:
            p = clean_price_text(tag.text)
            # é€™è£¡æˆ‘å€‘ç¨å¾®æ”¾å¯¬ä¸‹é™ï¼Œä½†åš´æ ¼éæ¿¾ä¸Šé™ (å¤ªå¤§çš„å¯èƒ½æ˜¯ç´…åˆ©é»æ•¸)
            if p and p > 50 and p < 200000:
                # print(f"ğŸ¯ å‘½ä¸­ä¿ƒéŠ·å€å¡Š ({sel}): {p}")
                return p, title

    # === ç­–ç•¥ B: JSON-LD (Level 2 - æ¬¡è¦) ===
    # å¦‚æœç¶²é ä¸Šæ‰¾ä¸åˆ°ç‰¹åƒ¹ CSSï¼Œæ‰å›é ­çœ‹ JSON-LD
    # ä½†é€™è£¡è¦å°å¿ƒï¼ŒJSON-LD å¯èƒ½æ˜¯åŸåƒ¹
    json_data = extract_json_ld(soup, "momo")
    if json_data:
        if 'name' in json_data and title == "Momoå•†å“": title = json_data['name']
        
        json_price = None
        if 'offers' in json_data:
            offers = json_data['offers']
            if isinstance(offers, dict) and 'price' in offers:
                json_price = clean_price_text(offers['price'])
            elif isinstance(offers, list):
                # å¦‚æœæœ‰å¤šå€‹ offer (ä¾‹å¦‚æœ‰ä½åƒ¹å’Œé«˜åƒ¹)ï¼Œé¸æœ€ä½çš„ï¼
                prices = []
                for offer in offers:
                    if 'price' in offer:
                        v = clean_price_text(offer['price'])
                        if v: prices.append(v)
                if prices:
                    json_price = min(prices)
        
        if json_price and json_price > 50:
             # print(f"ğŸ¯ å‘½ä¸­ JSON-LD: {json_price}")
             return json_price, title

    # === ç­–ç•¥ C: å»£æ³›æœå°‹ (Level 3 - ä¿åº•) ===
    # å¦‚æœä¸Šé¢éƒ½æ²’æŠ“åˆ°ï¼Œæƒææ‰€æœ‰å¯èƒ½æ˜¯åƒ¹æ ¼çš„åœ°æ–¹ï¼Œç„¶å¾Œå– "æœ€å°å€¼" (ä½†è¦å¤§æ–¼100)
    # åŸç†ï¼šå¦‚æœæœ‰ "3980" å’Œ "3680" åŒæ™‚å‡ºç¾ï¼Œæˆ‘å€‘æƒ³è¦ 3680
    candidates = []
    
    # æ”¶é›†æ‰€æœ‰ class="price"
    price_tags = soup.select(".price, .seoPrice")
    for tag in price_tags:
        # æ’é™¤è¢«åŠƒæ‰çš„åƒ¹æ ¼ (åŸåƒ¹)
        if "strike" in tag.get("class", []) or tag.find_parent("del"):
            continue
            
        p = clean_price_text(tag.text)
        if p and p > 100:
            candidates.append(p)
            
    # æ”¶é›† HTML è£¡é¢çš„æ•¸å­—
    html_str = str(soup)
    matches = re.findall(r'price[^>]*>.*?(\d{1,3}(?:,\d{3})*)', html_str)
    for m in matches:
        p = clean_price_text(m)
        if p and p > 100:
            candidates.append(p)
            
    if candidates:
        # éæ¿¾æ‰å¤ªå¤§çš„ (é¿å… 15000 æ»¿é¡è´ˆ)
        # å‡è¨­ä¸€èˆ¬å•†å“ä¸æœƒè¶…é 50è¬ (é™¤éä½ çœŸçš„æ˜¯è³£è»Š)
        valid_candidates = [c for c in candidates if c < 500000]
        
        if valid_candidates:
            # â˜… é—œéµæ”¹è®Šï¼šå–æœ€å°å€¼ï¼ (Assume Lowest Price is the Promo Price)
            # åœ¨æ’é™¤æ‰ < 100 çš„é›œè¨Šå¾Œï¼Œæœ€å°çš„é€šå¸¸æ˜¯ä¿ƒéŠ·åƒ¹
            best_price = min(valid_candidates)
            # print(f"ğŸ¯ å‘½ä¸­å€™é¸åƒ¹æ ¼æœ€å°å€¼: {best_price}")
            return best_price, title

    return None, title

def parse_pchome(soup):
    price, title = None, "PChomeå•†å“"
    # PChome é‚è¼¯: å„ªå…ˆæ‰¾ "ç›®å‰å”®åƒ¹" å€å¡Š
    
    # 1. è¦–è¦ºå€å¡Š (PChome çš„åƒ¹æ ¼ ID å¾ˆæ˜ç¢º)
    selectors = ["#PriceTotal", ".o-prodPrice__price", ".price-info__price"]
    for sel in selectors:
        tag = soup.select_one(sel)
        if tag:
            p = clean_price_text(tag.text)
            if p and p > 10: return p, title

    # 2. JSON-LD
    json_data = extract_json_ld(soup, "pchome")
    if json_data:
        if 'name' in json_data: title = json_data['name']
        if 'offers' in json_data:
            offers = json_data['offers']
            raw_p = None
            if isinstance(offers, dict) and 'price' in offers: raw_p = offers['price']
            elif isinstance(offers, list) and offers and 'price' in offers[0]: raw_p = offers[0]['price']
            
            p = clean_price_text(raw_p)
            if p and p > 10: return p, title

    return price, title

# --- 4. æ ¸å¿ƒåŠŸèƒ½: æŠ“å–å–®ä¸€å•†å“ ---

def get_product_info(url_or_base64):
    decoded_text = decode_base64_safe(url_or_base64)
    raw_url = extract_url_from_text(decoded_text)
    real_url = resolve_short_url(raw_url)
    
    if not real_url:
        return None, None, None

    print(f"ğŸ” çˆ¬å–: {real_url[:60]}...")
    
    platform = "unknown"
    if "momoshop.com.tw" in real_url: platform = "momo"
    elif "pchome.com.tw" in real_url: platform = "pchome"

    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled") 
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")

    price, title = None, None
    driver = webdriver.Chrome(options=chrome_options)
    try:
        driver.get(real_url)
        time.sleep(3) 
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        if platform == "momo": price, title = parse_momo(soup)
        elif platform == "pchome": price, title = parse_pchome(soup)
        else: price, title = parse_momo(soup)
    except Exception as e:
        print(f"âŒ çˆ¬èŸ²éŒ¯èª¤: {e}")
    finally:
        driver.quit()

    # ä¿åº•
    if (not price) and decoded_text and (len(decoded_text) < 1000):
        fallback_price = extract_price_from_user_text(decoded_text)
        if fallback_price:
            price = fallback_price
            print(f"âœ… æ–‡å­—ä¿åº•åƒ¹æ ¼: {price}")
            if not title or title == "Momoå•†å“":
                title = decoded_text.split('\n')[0][:50] 

    return price, title, real_url

# --- 5. è³‡æ–™åº«æ“ä½œ ---

def save_price_record(user_id, raw_input, price, title, url):
    if not supabase: return
    print(f"ğŸ’¾ å­˜æª”: {title} | ${price}")
    try:
        product_data = {
            "user_id": user_id,
            "original_url": url,
            "current_price": price,
            "product_name": title,
            "is_active": True,
            "updated_at": "now()"
        }
        existing = supabase.table("products").select("id").eq("original_url", url).eq("user_id", user_id).execute()
        
        if existing.data:
            pid = existing.data[0]['id']
            # V10.18: æ¯æ¬¡æŠ“å–éƒ½æ›´æ–° current_priceï¼Œç¢ºä¿æ˜¯æœ€æ–°
            supabase.table("products").update(product_data).eq("id", pid).execute()
        else:
            res = supabase.table("products").insert(product_data).execute()
            pid = res.data[0]['id'] if res.data else None

        if pid:
            supabase.table("price_history").insert({"product_id": pid, "price": price, "recorded_at": "now()"}).execute()
            print("âœ… æˆåŠŸ")
    except Exception as e:
        print(f"âŒ å¯«å…¥å¤±æ•—: {e}")

def check_all_products():
    if not supabase: return
    print("ğŸš€ å•Ÿå‹•å…¨åº«æƒææ¨¡å¼ (Cron Job)...")
    
    try:
        response = supabase.table("products").select("*").eq("is_active", True).execute()
        products = response.data
    except Exception as e:
        print(f"âŒ è®€å–è³‡æ–™åº«å¤±æ•—: {e}")
        return

    if not products:
        print("ğŸ“¦ ç›®å‰æ²’æœ‰å•†å“éœ€è¦æª¢æŸ¥")
        return

    print(f"ğŸ“¦ å…±æœ‰ {len(products)} å€‹å•†å“å¾…æª¢æŸ¥")
    
    for prod in products:
        pid = prod['id']
        p_url = prod['original_url']
        p_name = prod['product_name']
        old_price = prod['current_price']
        
        print(f"---------------------------------------------------")
        print(f"ğŸ” æª¢æŸ¥: {p_name[:15]}... (åŸåƒ¹: {old_price})")
        
        new_price, new_title, clean_url = get_product_info(p_url)
        
        if new_price:
            print(f"ğŸ’° æœ€æ–°åƒ¹æ ¼: {new_price}")
            
            # V10.18: æ›´åš´æ ¼çš„æ›´æ–°é‚è¼¯
            # å¦‚æœæŠ“åˆ°çš„åƒ¹æ ¼æ¯”åŸåƒ¹ä½ï¼Œæˆ–ä¸åŒï¼Œæˆ‘å€‘éƒ½æ›´æ–°
            # ä½†è¦é¿å…æŠ“åˆ° 0 æˆ– æ¥µå°å€¼
            if new_price != old_price and new_price > 50:
                supabase.table("products").update({
                    "current_price": new_price,
                    "updated_at": "now()"
                }).eq("id", pid).execute()

                supabase.table("price_history").insert({
                    "product_id": pid, 
                    "price": new_price, 
                    "recorded_at": "now()"
                }).execute()
                
                if new_price < old_price:
                    diff = old_price - new_price
                    print(f"ğŸ‰ é™åƒ¹äº†ï¼ ä¾¿å®œäº† ${diff} ({old_price} -> {new_price})")
        else:
            print("âš ï¸ ç„¡æ³•æŠ“å–åƒ¹æ ¼ï¼Œè·³é")
        
        time.sleep(5)

if __name__ == "__main__":
    if len(sys.argv) > 2:
        raw_msg = sys.argv[1]
        uid = sys.argv[2]
        print("ğŸš€ V10.18 è¦–è¦ºä¿ƒéŠ·å„ªå…ˆç‰ˆå•Ÿå‹•...")
        price, title, clean_url = get_product_info(raw_msg)
        if price:
            save_price_record(uid, raw_msg, price, title, clean_url)
        else:
            print("âŒ å¤±æ•—: ç„¡æ³•æŠ“å–")
    else:
        check_all_products()
