import os
import sys
import re
import time
import json
import base64
import requests
from urllib.parse import urlparse, parse_qs, unquote
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# --- 1. åˆå§‹åŒ– ---
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")

try:
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("âš ï¸ è­¦å‘Š: æœªåµæ¸¬åˆ° Supabase ç’°å¢ƒè®Šæ•¸")
        supabase = None
    else:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
except Exception as e:
    print(f"âŒ Supabase åˆå§‹åŒ–å¤±æ•—: {e}")
    sys.exit(1)

# --- 2. å·¥å…·å‡½å¼ ---

def decode_base64_safe(data):
    if not data: return ""
    try:
        return base64.b64decode(data).decode('utf-8')
    except:
        return data

def extract_inner_url(url):
    if not url: return None
    if "goodsUrl=" in url:
        try:
            parsed = urlparse(url)
            params = parse_qs(parsed.query)
            if 'goodsUrl' in params:
                return unquote(params['goodsUrl'][0])
        except: pass
    return url

def normalize_momo_url(url):
    if not url: return None
    match = re.search(r'goodsDetail/([A-Za-z0-9]+)', url)
    if match:
        product_id = match.group(1)
        if product_id.startswith("TP"): return url # TP ä¿æŒåŸæ¨£
        if product_id.isdigit():
            return f"https://www.momoshop.com.tw/goods/GoodsDetail.jsp?i_code={product_id}"
    return url

def resolve_short_url(url):
    if not url: return None
    if "momoshop.com.tw/goods/GoodsDetail" in url and "reurl.jsp" not in url:
        return url
    print(f"ğŸ”„ æ­£åœ¨é‚„åŸçŸ­ç¶²å€: {url} ...")
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10, allow_redirects=True)
        final_url = response.url
        inner_url = extract_inner_url(final_url)
        return normalize_momo_url(inner_url)
    except Exception as e:
        print(f"âš ï¸ é‚„åŸç¶²å€å¤±æ•—: {e}")
        return url

def extract_url_from_text(decoded_text):
    if not decoded_text: return None
    match = re.search(r'(https?://[^\s]+)', decoded_text)
    if match: return match.group(1)
    return decoded_text

def clean_price_text(text):
    """æ¸…æ´—åƒ¹æ ¼æ–‡å­—ï¼Œä¸¦éæ¿¾ä¸åˆç†çš„ä½åƒ¹"""
    if not text: return None
    # åªç•™æ•¸å­—
    clean = re.sub(r'[^\d]', '', str(text))
    
    if not clean: return None
    
    price = int(clean)
    
    # â˜… V10.14 æ ¸å¿ƒä¿®æ­£ï¼šä½åƒ¹éæ¿¾å™¨
    # å¦‚æœåƒ¹æ ¼å°æ–¼ 10 å…ƒï¼Œæ¥µæœ‰å¯èƒ½æ˜¯ã€Œ1å…¥ã€ã€ã€Œ1ä»¶ã€æˆ–ã€Œ0å…ƒé‹è²»ã€ï¼Œç›´æ¥å¿½ç•¥
    if price < 10:
        # print(f"âš ï¸ å¿½ç•¥ä¸åˆç†ä½åƒ¹: {price}")
        return None
        
    return price

def extract_price_from_user_text(text):
    """æ–‡å­—ä¿åº•æ©Ÿåˆ¶ (å«ä½åƒ¹éæ¿¾)"""
    if not text: return None
    print("ğŸ›¡ï¸ å•Ÿå‹•ä¿åº•æ©Ÿåˆ¶: å˜—è©¦å¾æ–‡å­—æå–åƒ¹æ ¼...")
    
    # ç­–ç•¥ A: æ‰¾ ã€xxxxå…ƒã€‘
    matches = re.finditer(r'ã€(\d+(?:,\d+)*)å…ƒ', text)
    for m in matches:
        p = clean_price_text(m.group(1))
        if p: return p
    
    # ç­–ç•¥ B: æ‰¾ $xxxx
    matches = re.finditer(r'\$(\d+(?:,\d+)*)', text)
    for m in matches:
        p = clean_price_text(m.group(1))
        if p: return p
        
    # ç­–ç•¥ C: æ‰¾ xxxxå…ƒ (æœ€å¯¬é¬†)
    matches = re.finditer(r'(\d+(?:,\d+)*)å…ƒ', text)
    for m in matches:
        p = clean_price_text(m.group(1))
        if p: return p
    
    return None

def extract_json_ld(soup, platform):
    scripts = soup.find_all('script', type='application/ld+json')
    for script in scripts:
        try:
            data = json.loads(script.string)
            if isinstance(data, list):
                for item
